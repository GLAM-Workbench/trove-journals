{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get OCRd text from a digitised journal in Trove\n",
    "\n",
    "Many of the digitised journals available in Trove make OCRd text available for download – one text file for each journal issue. However, while there are records for journals and articles in Trove (and available through the API), there are no records for issues. So how do we find them?\n",
    "\n",
    "After a bit of poking around I noticed that the landing page for a journal calls an internal API to deliver the HTML content of the 'Browse' panel. This browse panel includes links to all the issues of the journal. The API that populates it takes a `startIdx` parameter and returns a maximum of 20 issues. Using this you can work your way through the complete list of issues, scraping the basic metadata from the HTML, including the identifier, title, and number of pages.\n",
    "\n",
    "Once we have a list of issues we can use the issue identifiers and page numbers to construct urls to download all the OCRd text files.\n",
    "\n",
    "In my first attempt at this, I [harvested metadata and OCRd text from *The Bulletin*](Harvesting-data-from-the-Bulletin.ipynb). In that notebook I included some additional steps to parse the issue metadata, extracting dates and issue numbers. That works fine for *The Bulletin*, but there's a lot of variation in the way issue details are expressed. To make it possible to use this notebook with any journal, I've just saved the details as a string. Depending on the journal and what you want to do with the data, you might want to parse this string to extract more useful metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What journal do you want?\n",
    "\n",
    "In the cell below, replace the `nla.obj-...` value with the identifier of the journal you want to harvest. You'll find the identifier in the url of the journal's landing page. An easy way to find it is to go to the [Trove Titles app](https://trove-titles.herokuapp.com/) and click on the 'Browse issues' button for the journal you're interested in.\n",
    "\n",
    "For example, if I click on the 'Browse issues' button for the *Angry Penguins broadsheet* it opens `http://nla.gov.au/nla.obj-320790312`, so the journal identifier is `nla.obj-320790312`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the value in the single quotes with the identifier of your chosen journal\n",
    "journal_id = 'nla.obj-320790312'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the libraries we need.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from slugify import slugify\n",
    "from IPython.display import display, HTML, FileLink\n",
    "\n",
    "s = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[ 502, 503, 504 ])\n",
    "s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "s.mount('http://', HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions to do the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_metadata(obj_id):\n",
    "    '''\n",
    "    This calls an internal API from a journal landing page to extract a list of available issues.\n",
    "    '''\n",
    "    start_url = 'https://nla.gov.au/{}/browse?startIdx={}&rows=20&op=c'\n",
    "    # The initial startIdx value\n",
    "    start = 0\n",
    "    # Number of results per page\n",
    "    n = 20\n",
    "    issues = []\n",
    "    with tqdm(desc='Issues', leave=False) as pbar:\n",
    "        # If there aren't 20 results on the page then we've reached the end, so continue harvesting until that happens.\n",
    "        while n == 20:\n",
    "            # Get the browse page\n",
    "            response = s.get(start_url.format(obj_id, start), timeout=60)\n",
    "            # Beautifulsoup turns the HTML into an easily navigable structure\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            # Find all the divs containing issue details and loop through them\n",
    "            details = soup.find_all(class_='l-item-info')\n",
    "            for detail in details:\n",
    "                issue = {}\n",
    "                title = detail.find('h3')\n",
    "                if title:\n",
    "                    issue['title'] = title.text\n",
    "                    issue['id'] = title.parent['href'].strip('/')\n",
    "                else:\n",
    "                    issue['title'] = 'No title'\n",
    "                    issue['id'] = detail.find('a')['href'].strip('/')\n",
    "                try:\n",
    "                    # Get the issue details\n",
    "                    issue['details'] = detail.find(class_='obj-reference content').string.strip()\n",
    "                except (AttributeError, IndexError):\n",
    "                    issue['details'] = 'issue'\n",
    "                # Get the number of pages\n",
    "                try:\n",
    "                    issue['pages'] = int(re.search(r'^(\\d+)', detail.find('a', attrs={'data-pid': issue['id']}).text, flags=re.MULTILINE).group(1))\n",
    "                except AttributeError:\n",
    "                    issue['pages'] = 0\n",
    "                issues.append(issue)\n",
    "                #print(issue)\n",
    "                time.sleep(0.2)\n",
    "            # Increment the startIdx\n",
    "            start += n\n",
    "            # Set n to the number of results on the current page\n",
    "            n = len(details)\n",
    "            pbar.update(n)\n",
    "    return issues\n",
    "\n",
    "def save_ocr(issues, obj_id, title=None, output_dir='journals'):\n",
    "    '''\n",
    "    Download the OCRd text for each issue.\n",
    "    '''\n",
    "    processed_issues = []\n",
    "    if not title:\n",
    "        title = issues[0]['title']\n",
    "    output_path = os.path.join(output_dir, '{}-{}'.format(slugify(title)[:50], obj_id))\n",
    "    texts_path = os.path.join(output_path, 'texts')\n",
    "    os.makedirs(texts_path, exist_ok=True)\n",
    "    for issue in tqdm(issues, desc='Texts', leave=False):\n",
    "        # Default values\n",
    "        issue['text_file'] = ''\n",
    "        if issue['pages'] != 0:       \n",
    "            # print(book['title'])\n",
    "            # The index value for the last page of an issue will be the total pages - 1\n",
    "            last_page = issue['pages'] - 1\n",
    "            file_name = '{}-{}-{}.txt'.format(slugify(issue['title'])[:50], slugify(issue['details'])[:50], issue['id'])\n",
    "            file_path = os.path.join(texts_path, file_name)\n",
    "            # Check to see if the file has already been harvested\n",
    "            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "                # print('Already saved')\n",
    "                issue['text_file'] = file_name\n",
    "            else:\n",
    "                url = 'https://trove.nla.gov.au/{}/download?downloadOption=ocr&firstPage=0&lastPage={}'.format(issue['id'], last_page)\n",
    "                # print(url)\n",
    "                # Get the file\n",
    "                r = s.get(url, timeout=120)\n",
    "                # Check there was no error\n",
    "                if r.status_code == requests.codes.ok:\n",
    "                    # Check that the file's not empty\n",
    "                    r.encoding = 'utf-8'\n",
    "                    if len(r.text) > 0 and not r.text.isspace():\n",
    "                        # Check that the file isn't HTML (some not found pages don't return 404s)\n",
    "                        if BeautifulSoup(r.text, 'html.parser').find('html') is None:\n",
    "                            # If everything's ok, save the file\n",
    "                            with open(file_path, 'w', encoding='utf-8') as text_file:\n",
    "                                text_file.write(r.text)\n",
    "                            issue['text_file'] = file_name\n",
    "                time.sleep(1)\n",
    "        processed_issues.append(issue)\n",
    "    df = pd.DataFrame(processed_issues)\n",
    "    # Remove empty directories\n",
    "    try:\n",
    "        os.rmdir(texts_path)\n",
    "        os.rmdir(output_path)\n",
    "    except OSError:\n",
    "        #It's not empty, so add list of issues\n",
    "        df.to_csv(os.path.join(output_path, '{}-issues.csv'.format(obj_id)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of issues\n",
    "\n",
    "Run the cell below to extract a list of issues for your selected journal and save them to the `issues` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = harvest_metadata(journal_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the OCRd texts\n",
    "\n",
    "Now we have the issues, we can download the texts!\n",
    "\n",
    "The OCRd text for each issue will be saved in an individual text file. By default, results will be saved under the `journals` directory, though you can change this by giving the `save_ocr()` function a different value for `output_dir`.\n",
    "\n",
    "The name of the journal directory is created using the journal title and journal id. Inside this directory is a CSV formatted file containing details of all the available issues, and a `texts` sub-directory to contain the downloaded text files.\n",
    "\n",
    "The individual file names are created using the journal title, issue details, and issue identifier. So the resulting hierarchy might look something like this:\n",
    "\n",
    "```\n",
    "journals\n",
    "    - angry-penguins-nla.obj-320790312\n",
    "        - nla.obj-320790312-issues.csv\n",
    "        - texts\n",
    "            - angry-penguins-broadsheet-no-1-nla.obj-320791009.txt\n",
    "```\n",
    "\n",
    "The CSV list of issues includes the following fields:\n",
    "\n",
    "* `details` – string with issue details, might include dates, issue numbers etc.\n",
    "* `id` – issue identifier\n",
    "* `pages` – number of pages in this issue\n",
    "* `text_file` – file name of any downloaded OCRd text\n",
    "* `title` – journal title (as extracted from issue browse list, might differ from original journal title)\n",
    "\n",
    "Note that if the `text_file` field is empty, it means that no OCRd text could be extracted for that particular issue. Note also that if no OCRd text is available, no journal directory will be created, and nothing will be saved.\n",
    "\n",
    "Run the cell below to download the OCRd text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ocr(issues, journal_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View and download the results\n",
    "\n",
    "If you've used the default output directory, you'll find the data in the `journals` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this notebook using a cloud service (like Binder), you'll want to download your results. The cell below zips up the journal directory and creates a link for easy download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_dir = glob.glob(os.path.join('journals', '*-{}'.format(journal_id)))[0]\n",
    "shutil.make_archive(journal_dir, 'zip', journal_dir)\n",
    "display(HTML('<b>Download results</b>'))\n",
    "display(HTML(f'<a download=\"{os.path.basename(journal_dir)}.zip\" href=\"{journal_dir}.zip\">{journal_dir}.zip</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a peek at the issues data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(journal_dir, '{}-issues.csv'.format(journal_id)), keep_default_na=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many issues are available, and how many have OCRd text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_issues = df.shape[0]\n",
    "num_text = df.loc[df['text_file'] != ''].shape[0]\n",
    "print( '{} / {} issues have OCRd text'.format(num_issues, num_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) for the [GLAM Workbench](https://glam-workbench.github.io/).\n",
    "\n",
    "Work on this notebook was supported by the [Humanities, Arts and Social Sciences (HASS) Data Enhanced Virtual Lab](https://tinker.edu.au/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
