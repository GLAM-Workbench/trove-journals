{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca7e8de-58b9-4e5a-a7ce-105481f8ad07",
   "metadata": {},
   "source": [
    "# Download issues of a periodical as PDFs\n",
    "\n",
    "This notebook helps you download the issues of a digitised periodical as PDFs. You can download all digitised issues, or specify a range of years to include.\n",
    "\n",
    "There are three main steps:\n",
    "\n",
    "- get a list of all the `nla.obj` identifiers of the periodical's issues\n",
    "- get the number of pages in each issue\n",
    "- construct a url to download each issue as a PDF using the `nla.obj` identifier and the number of pages\n",
    "\n",
    "Depending on the periodical, this could take many hours to complete and consume a lot of disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bfee23-4458-4e53-85ac-12f3ad9879cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the libraries we need.\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import arrow\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "s = requests_cache.CachedSession(expire_after=timedelta(days=30))\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[502, 503, 504])\n",
    "s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "s.mount(\"http://\", HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54470166-85a8-4a82-91a9-2bf019ebedeb",
   "metadata": {},
   "source": [
    "## Set your parameters\n",
    "\n",
    "Edit the cell below to insert the `nla.obj` identifier of the periodical. This identifier will point to the top-level collection page of a periodical in Trove's digitised object viewer. For example, the url of the top-level page of *The Bulletin* is <https://nla.gov.au/nla.obj-68375465>, so the identifier is `nla.obj-68375465`. If you're viewing a digitised issue or page within a periodical, you can use the breadcrumbs link to navigate up to the top-level page.\n",
    "\n",
    "Finding digitised periodicals in Trove is not always easy – the Trove Data Guide [provides some hints](https://tdg.glam-workbench.net/other-digitised-resources/periodicals/finding-periodicals.html). You can also search this [database of digitised periodical titles](https://glam-workbench.net/datasette-lite/?url=https://github.com/GLAM-Workbench/trove-periodicals-data/blob/main/periodicals.db&install=datasette-json-html&install=datasette-template-sql&metadata=https://github.com/GLAM-Workbench/trove-periodicals-data/blob/main/metadata.json#/periodicals/titles).\n",
    "\n",
    "By default, this notebook will download *all* the issues of a periodical. If you only want issues from a particular range of years, set the `start_year` and/or `end_year` values in the cell below. For exammple, setting `start_year = 1900` and `end_year = 1940` will download all issues published between 1900 and 1940.\n",
    "\n",
    "Once you've made your changes to the cell below, select 'Run > Run All Cells' from the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac3af9-4cc7-4734-a48a-08d05263995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the periodical's nla.obj identifier\n",
    "periodical_id = \"nla.obj-8423556\"\n",
    "\n",
    "# Optionally set a range of years\n",
    "start_year = None\n",
    "end_year = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d08a6-1aa6-41c1-9616-3adc8550c75e",
   "metadata": {},
   "source": [
    "## Get issue identifiers\n",
    "\n",
    "Version 3 of the Trove API added a new endpoint to [provide information about periodical titles and issues](https://tdg.glam-workbench.net/other-digitised-resources/periodicals/accessing-data.html#using-the-magazine-titles-api-endpoint). However, the issues data provided by the API is incomplete. A more reliable alternative is to scrape the list of issues from the browse window in the digitised object viewer – see [HOW TO: Get a list of items from a digitised collection](https://tdg.glam-workbench.net/other-digitised-resources/how-to/get-collection-items.html) in the *Trove Data Guide*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71793d17-1d9e-4eae-9435-6582a2c509f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_ids(periodical_id):\n",
    "    # The initial startIdx value\n",
    "    start = 0\n",
    "    # Number of results per page, used to increment the startIdx value\n",
    "    n = 20\n",
    "\n",
    "    items = []\n",
    "    with tqdm() as pbar:\n",
    "        # If there aren't 20 results on the page then we've reached the end, so continue harvesting until that happens.\n",
    "        while n == 20:\n",
    "            url = f\"https://nla.gov.au/{periodical_id}/browse?startIdx={start}&rows=20&op=c\"\n",
    "\n",
    "            # Get the browse page\n",
    "            response = s.get(url)\n",
    "\n",
    "            # Beautifulsoup turns the HTML into an easily navigable structure\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find all the divs containing issue details and loop through them\n",
    "            details = soup.find_all(class_=\"l-item-info\")\n",
    "            for detail in details:\n",
    "                # Look for the a tag with class \"obj-reference content\"\n",
    "                item_id = detail.find(\n",
    "                    lambda tag: tag.name == \"a\"\n",
    "                    and tag.get(\"class\") == [\"obj-reference\", \"content\"]\n",
    "                )[\"href\"].strip(\"/\")\n",
    "\n",
    "                # Save the issue id\n",
    "                items.append(item_id)\n",
    "            if not response.from_cache:\n",
    "                time.sleep(0.2)\n",
    "            # Increment the startIdx\n",
    "            start += n\n",
    "            # Set n to the number of results on the current page\n",
    "            n = len(details)\n",
    "            pbar.update(n)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08f164-f421-4010-9d9c-13dd95fc4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_ids = get_issue_ids(periodical_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581f9ae-d6cc-4a66-8c1b-f77701f718c6",
   "metadata": {},
   "source": [
    "## Get number of pages in each issue\n",
    "\n",
    "It's possible to scrape the number of pages along with the identifiers in the previous step. However, I'm not certain that the information is displayed consistently across all periodicals. To play it safe, I'm extracting embedded metadata from the digitised object viewer and getting the number of pages, issue dates, and publication details (if available). See [HOW TO: Extract additional metadata from the digitised resource viewer](https://tdg.glam-workbench.net/other-digitised-resources/how-to/extract-embedded-metadata.html) in the *Trove Data Guide*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78126ce-e8ef-4984-aaf3-709c30a8ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(id):\n",
    "    \"\"\"\n",
    "    Extract work data in a JSON string from the work's HTML page.\n",
    "    \"\"\"\n",
    "    if not id.startswith(\"http\"):\n",
    "        id = \"https://nla.gov.au/\" + id\n",
    "    response = s.get(id)\n",
    "    try:\n",
    "        work_data = re.search(\n",
    "            r\"var work = JSON\\.parse\\(JSON\\.stringify\\((\\{.*\\})\", response.text\n",
    "        ).group(1)\n",
    "    except AttributeError:\n",
    "        work_data = \"{}\"\n",
    "    if not response.from_cache:\n",
    "        time.sleep(0.2)\n",
    "    return json.loads(work_data)\n",
    "\n",
    "\n",
    "def get_issue_data(issue_ids):\n",
    "    issues = []\n",
    "\n",
    "    for issue_id in tqdm(issue_ids):\n",
    "        metadata = get_metadata(issue_id)\n",
    "        date = metadata.get(\"issueDate\", \"\")\n",
    "        try:\n",
    "            iso_date = arrow.get(date, \"ddd, D MMM YYYY\").format(\"YYYY-MM-DD\")\n",
    "        except arrow.parser.ParserMatchError:\n",
    "            iso_date = \"\"\n",
    "        issue = {\n",
    "            \"id\": issue_id,\n",
    "            \"date\": date,\n",
    "            \"iso_date\": iso_date,\n",
    "            \"details\": metadata.get(\"subUnitNo\", \"\"),\n",
    "            \"pages\": len(metadata[\"children\"][\"page\"]),\n",
    "        }\n",
    "        issues.append(issue)\n",
    "\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a22530-6647-4fa0-b779-76c521814c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = get_issue_data(issue_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c565a4-1754-40a9-af9b-2551f984e3af",
   "metadata": {},
   "source": [
    "## Download PDFs\n",
    "\n",
    "Once we have the identifier and number of pages we can construct a url to download each issue. See: [HOW TO: Get text, images, and PDFs using Trove’s download link](https://tdg.glam-workbench.net/other-digitised-resources/how-to/get-downloads.html) in the *Trove Data Guide*.\n",
    "\n",
    "The downloaded PDFs will be saved in the `pdfs` directory, within a subdirectory named using the periodical's `nla.obj` identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc955bca-411d-4409-85b4-6e4fad4df050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_issues(issues, start_year=None, end_year=None):\n",
    "    filtered = []\n",
    "    if not (start_year or end_year):\n",
    "        return issues\n",
    "    for issue in issues:\n",
    "        year = issue[\"iso_date\"][:4]\n",
    "        if year:\n",
    "            year = int(year)\n",
    "            if start_year and end_year:\n",
    "                if year >= start_year and year <= end_year:\n",
    "                    filtered.append(issue)\n",
    "            elif start_year:\n",
    "                if year >= start_year:\n",
    "                    filtered.append(issue)\n",
    "            elif end_year:\n",
    "                if year <= end_year:\n",
    "                    filtered.append(issue)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def download_pdfs(issues, start_year=None, end_year=None):\n",
    "    output_dir = Path(\"pdfs\", periodical_id)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for issue in tqdm(filter_issues(issues, start_year, end_year)):\n",
    "        pdf_url = f\"https://nla.gov.au/{issue['id']}/download?downloadOption=pdf&firstPage=0&lastPage={issue['pages']-1}\"\n",
    "        response = s.get(pdf_url, stream=True)\n",
    "        if issue[\"iso_date\"]:\n",
    "            filename = f\"{issue['iso_date']}-{issue['id']}.pdf\"\n",
    "        Path(output_dir, filename).write_bytes(response.content)\n",
    "        if not response.from_cache:\n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb182f-a707-49ea-89a3-c059a249963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_pdfs(issues, start_year, end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e179894-8885-479b-9658-99a4c5af69e8",
   "metadata": {},
   "source": [
    "## Save metadata\n",
    "\n",
    "You can also save the harvested issue metadata as a CSV file. It will be saved in the same directory as the PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf06f1f-3083-4213-a042-e4855fc6c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_parts = [str(p) for p in [periodical_id, \"issues\", start_year, end_year] if p]\n",
    "csv_filename = f\"{'-'.join(name_parts)}.csv\"\n",
    "df = pd.DataFrame(issues)\n",
    "df.to_csv(Path(\"pdfs\", periodical_id, csv_filename, index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eddc03-af37-46ab-9366-2403dba15000",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.au/) for the [GLAM Workbench](https://glam-workbench.net/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rocrate": {
   "author": [
    {
     "mainEntityOfPage": "https://timsherratt.au",
     "name": "Sherratt, Tim",
     "orcid": "https://orcid.org/0000-0001-7956-4498"
    }
   ],
   "category": "Harvesting images",
   "description": "This notebook helps you download the issues of a digitised periodical as PDFs. You can download all digitised issues, or specify a range of years to include.",
   "mainEntityOfPage": "https://glam-workbench.net/trove-journals/save-issues-as-pdfs/",
   "name": "Download issues of a periodical as PDFs",
   "position": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
