{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the OCRd text for ALL the digitised periodicals in Trove!\n",
    "\n",
    "Putting together the [list of periodicals](digital-journals.csv) [created by this notebook](Create-digitised-journals-list.ipynb) with the [code in this notebook](Get-text-from-a-Trove-journal.ipynb), you can download the OCRd text from every digitised periodical in the `journals` zone. If you're going to try this, you'll need a lots of patience and lots of disk space. Needless to say, don't try this on a cloud service like Binder.\n",
    "\n",
    "Fortunately you don't have to do it yourself, as I've already run the harvest and made all the text files available. See below for details.\n",
    "\n",
    "I repeat, **you probably don't want to do this yourself**. The point of this notebook is really to document the methodology used to create the repository.\n",
    "\n",
    "If you really, really do want to do it yourself, you should first [generate an updated list of digitised periodicals](Create-digitised-journals-list.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's a harvest I prepared earlier...\n",
    "\n",
    "I last ran this harvest in August 2021. Here are the results:\n",
    "\n",
    "* 1,163 periodicals had OCRd text available for download\n",
    "* OCRd text was downloaded from 51,928 periodical issues \n",
    "* About 10gb of text was downloaded\n",
    "\n",
    "Note that, unlike previous harvests, this one excluded periodicals with the format 'government publication' â€“ so the total amount harvested has decreased. Government publications are actually spread across both the books and journals zone, so I'm planning to do a separate harvest just for them.\n",
    "\n",
    "The list of digital journals with OCRd text is available both as [human-readable list](digital-journals-with-text.md) and a [CSV formatted spreadsheet](digital-journals-with-text.csv).\n",
    "\n",
    "The complete collection of text files for all the journals can be downloaded [from this repository on CloudStor](https://cloudstor.aarnet.edu.au/plus/s/QOmnqpGQCNCSC2h)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the libraries we need.\n",
    "import requests\n",
    "import arrow\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from slugify import slugify\n",
    "from IPython.display import display, HTML, FileLink\n",
    "import requests_cache\n",
    "from pathlib import Path\n",
    "\n",
    "s = requests_cache.CachedSession()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[ 502, 503, 504 ])\n",
    "s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "s.mount('http://', HTTPAdapter(max_retries=retries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are copied from Get-text-from-a-Trove-journal.ipynb\n",
    "\n",
    "def harvest_metadata(obj_id):\n",
    "    '''\n",
    "    This calls an internal API from a journal landing page to extract a list of available issues.\n",
    "    '''\n",
    "    start_url = 'https://nla.gov.au/{}/browse?startIdx={}&rows=20&op=c'\n",
    "    # The initial startIdx value\n",
    "    start = 0\n",
    "    # Number of results per page\n",
    "    n = 20\n",
    "    issues = []\n",
    "    with tqdm(desc='Issues', leave=False) as pbar:\n",
    "        # If there aren't 20 results on the page then we've reached the end, so continue harvesting until that happens.\n",
    "        while n == 20:\n",
    "            # Get the browse page\n",
    "            response = s.get(start_url.format(obj_id, start), timeout=60)\n",
    "            # Beautifulsoup turns the HTML into an easily navigable structure\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            # Find all the divs containing issue details and loop through them\n",
    "            details = soup.find_all(class_='l-item-info')\n",
    "            for detail in details:\n",
    "                issue = {}\n",
    "                title = detail.find('h3')\n",
    "                if title:\n",
    "                    issue['title'] = title.text\n",
    "                    issue['id'] = title.parent['href'].strip('/')\n",
    "                else:\n",
    "                    issue['title'] = 'No title'\n",
    "                    issue['id'] = detail.find('a')['href'].strip('/')\n",
    "                try:\n",
    "                    # Get the issue details\n",
    "                    issue['details'] = detail.find(class_='obj-reference content').string.strip()\n",
    "                except (AttributeError, IndexError):\n",
    "                    issue['details'] = 'issue'\n",
    "                # Get the number of pages\n",
    "                try:\n",
    "                    issue['pages'] = int(re.search(r'^(\\d+)', detail.find('a', attrs={'data-pid': issue['id']}).text, flags=re.MULTILINE).group(1))\n",
    "                except AttributeError:\n",
    "                    issue['pages'] = 0\n",
    "                issues.append(issue)\n",
    "                # print(issue)\n",
    "                if not response.from_cache:\n",
    "                    time.sleep(0.5)\n",
    "            # Increment the startIdx\n",
    "            start += n\n",
    "            # Set n to the number of results on the current page\n",
    "            n = len(details)\n",
    "            pbar.update(n)\n",
    "    return issues\n",
    "\n",
    "def save_ocr(issues, obj_id, title=None, output_dir='journals'):\n",
    "    '''\n",
    "    Download the OCRd text for each issue.\n",
    "    '''\n",
    "    processed_issues = []\n",
    "    if not title:\n",
    "        title = issues[0]['title']\n",
    "    output_path = os.path.join(output_dir, '{}-{}'.format(slugify(title)[:50], obj_id))\n",
    "    texts_path = os.path.join(output_path, 'texts')\n",
    "    os.makedirs(texts_path, exist_ok=True)\n",
    "    for issue in tqdm(issues, desc='Texts', leave=False):\n",
    "        # Default values\n",
    "        issue['text_file'] = ''\n",
    "        if issue['pages'] != 0:       \n",
    "            # print(book['title'])\n",
    "            # The index value for the last page of an issue will be the total pages - 1\n",
    "            last_page = issue['pages'] - 1\n",
    "            file_name = '{}-{}-{}.txt'.format(slugify(issue['title'])[:50], slugify(issue['details'])[:50], issue['id'])\n",
    "            file_path = os.path.join(texts_path, file_name)\n",
    "            # Check to see if the file has already been harvested\n",
    "            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "                # print('Already saved')\n",
    "                issue['text_file'] = file_name\n",
    "            else:\n",
    "                url = 'https://trove.nla.gov.au/{}/download?downloadOption=ocr&firstPage=0&lastPage={}'.format(issue['id'], last_page)\n",
    "                # print(url)\n",
    "                # Get the file\n",
    "                r = s.get(url, timeout=120)\n",
    "                # Check there was no error\n",
    "                if r.status_code == requests.codes.ok:\n",
    "                    # Check that the file's not empty\n",
    "                    r.encoding = 'utf-8'\n",
    "                    if len(r.text) > 0 and not r.text.isspace():\n",
    "                        # Check that the file isn't HTML (some not found pages don't return 404s)\n",
    "                        if BeautifulSoup(r.text, 'html.parser').find('html') is None:\n",
    "                            # If everything's ok, save the file\n",
    "                            with open(file_path, 'w', encoding='utf-8') as text_file:\n",
    "                                text_file.write(r.text)\n",
    "                            issue['text_file'] = file_name\n",
    "                if not r.from_cache:\n",
    "                    time.sleep(1)\n",
    "        processed_issues.append(issue)\n",
    "    df = pd.DataFrame(processed_issues)\n",
    "    # Remove empty directories\n",
    "    try:\n",
    "        os.rmdir(texts_path)\n",
    "        os.rmdir(output_path)\n",
    "    except OSError:\n",
    "        #It's not empty, so add list of issues\n",
    "        df.to_csv(os.path.join(output_path, '{}-issues.csv'.format(obj_id)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all the journals!\n",
    "\n",
    "As already mentioned, this takes a long time. It will also probably fail at various points and you'll have to run it again. If you do restart, the script will start at the beginning, but won't redownload any text files have already been harvested.\n",
    "\n",
    "Results for each journal are saved in a separate directory in the outpur directory (which defaults to `journals`). The name of the journal directory is created using the journal title and journal id. Inside this directory is a CSV formatted file containing details of all the available issues, and a `texts` sub-directory to contain the downloaded text files.\n",
    "\n",
    "The individual file names are created using the journal title, issue details, and issue identifier. So the resulting hierarchy might look something like this:\n",
    "\n",
    "```\n",
    "journals\n",
    "    - angry-penguins-nla.obj-320790312\n",
    "        - nla.obj-320790312-issues.csv\n",
    "        - texts\n",
    "            - angry-penguins-broadsheet-no-1-nla.obj-320791009.txt\n",
    "```\n",
    "\n",
    "The CSV list of issues includes the following fields:\n",
    "\n",
    "* `details` â€“ string with issue details, might include dates, issue numbers etc.\n",
    "* `id` â€“ issue identifier\n",
    "* `pages` â€“ number of pages in this issue\n",
    "* `text_file` â€“ file name of any downloaded OCRd text\n",
    "* `title` â€“ journal title (as extracted from issue browse list, might differ from original journal title)\n",
    "\n",
    "Note that if the `text_file` field is empty, it means that no OCRd text could be extracted for that particular issue. Note also that if no OCRd text is available, no journal directory will be created, and nothing will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can provide a different output_dir if you want\n",
    "def process_titles(output_dir='journals'):\n",
    "    df = pd.read_csv('digital-journals.csv')\n",
    "    # df = pd.read_csv('government-publications-periodicals.csv')\n",
    "    # Drop duplicate records taking the records with nla digitised = True\n",
    "    journals = df.sort_values(by=['trove_id', 'fulltext_url_type']).drop_duplicates(subset='trove_id', keep='last').to_dict('records')\n",
    "    for journal in tqdm(journals, desc='Journals'):\n",
    "        issues = harvest_metadata(journal['trove_id'])\n",
    "        if issues:\n",
    "            save_ocr(issues, journal['trove_id'], title= journal['title'], output_dir=output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start harvesting!!!!\n",
    "process_titles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather data about the harvest\n",
    "\n",
    "Because the harvesting takes a long time and is prone to failure, it seemed wise to gather data at the end, rather than keeping a running total.\n",
    "\n",
    "The cells below create a list of journals that have OCRd text. The list has the following fields:\n",
    "\n",
    "* `fulltext_url` â€“ the url of the landing page of the digital version of the journal\n",
    "* `title` â€“ the title of the journal\n",
    "* `trove_id` â€“ the 'nla.obj' part of the fulltext_url, a unique identifier for the digital journal\n",
    "* `trove_url` â€“ url of the journal's metadata record in Trove\n",
    "* `issues` â€“ the number of available issues\n",
    "* `issues_with_text` â€“ the number of issues that OCRd text could be downloaded from\n",
    "* `directory` â€“ the directory in which the files from this journal have been saved (relative to the output directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_issue_data(output_path='journals'):\n",
    "    titles_with_text = []\n",
    "    df = pd.read_csv('digital-journals-20210805.csv', keep_default_na=False)\n",
    "    # df = pd.read_csv('government-publications-periodicals-20210802.csv')\n",
    "    journals = df.to_dict('records')\n",
    "    for j in journals:\n",
    "        j_dir = os.path.join(output_path, '{}-{}'.format(slugify(j['title'])[:50], j['trove_id']))\n",
    "        if os.path.exists(j_dir):\n",
    "            csv_file = os.path.join(j_dir, '{}-issues.csv'.format(j['trove_id']))\n",
    "            issues_df = pd.read_csv(csv_file, keep_default_na=False)\n",
    "            j['issues'] = issues_df.shape[0]\n",
    "            j['issues_with_text'] = issues_df.loc[issues_df['text_file'] != ''].shape[0]\n",
    "            j['directory'] = '{}-{}'.format(slugify(j['title'])[:50], j['trove_id'])\n",
    "            titles_with_text.append(j)\n",
    "    return titles_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the data\n",
    "#titles_with_text = collect_issue_data()\n",
    "titles_with_text = collect_issue_data('/Volumes/bigdata/mydata/Trove/journals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>contributor</th>\n",
       "      <th>issued</th>\n",
       "      <th>format</th>\n",
       "      <th>fulltext_url</th>\n",
       "      <th>trove_url</th>\n",
       "      <th>trove_id</th>\n",
       "      <th>fulltext_url_type</th>\n",
       "      <th>issues</th>\n",
       "      <th>issues_with_text</th>\n",
       "      <th>directory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Laws, etc. (Acts of the Parliament)</td>\n",
       "      <td>Victoria</td>\n",
       "      <td>1900-2021</td>\n",
       "      <td>Government publication | Periodical | Periodic...</td>\n",
       "      <td>http://nla.gov.au/nla.obj-54127737</td>\n",
       "      <td>https://trove.nla.gov.au/work/10078182</td>\n",
       "      <td>nla.obj-54127737</td>\n",
       "      <td>digitised</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>laws-etc-acts-of-the-parliament-nla.obj-54127737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Silver stream songster</td>\n",
       "      <td></td>\n",
       "      <td>1890-1900</td>\n",
       "      <td>Periodical | Periodical/Journal, magazine, other</td>\n",
       "      <td>https://nla.gov.au/nla.obj-614066685</td>\n",
       "      <td>https://trove.nla.gov.au/work/10087062</td>\n",
       "      <td>nla.obj-614066685</td>\n",
       "      <td>digitised</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>the-silver-stream-songster-nla.obj-614066685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Report / Defence Force Remuneration Tribunal</td>\n",
       "      <td>Australia. Defence Force Remuneration Tribunal</td>\n",
       "      <td>1980-2021</td>\n",
       "      <td>Government publication | Periodical | Periodic...</td>\n",
       "      <td>https://nla.gov.au/nla.obj-2137302489</td>\n",
       "      <td>https://trove.nla.gov.au/work/10096343</td>\n",
       "      <td>nla.obj-2137302489</td>\n",
       "      <td>digitised</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>report-defence-force-remuneration-tribunal-nla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Territory of Papua : annual report for the per...</td>\n",
       "      <td>Australia. Department of External Territories</td>\n",
       "      <td>1940-1949</td>\n",
       "      <td>Government publication | Periodical | Periodic...</td>\n",
       "      <td>https://nla.gov.au/nla.obj-2060262652</td>\n",
       "      <td>https://trove.nla.gov.au/work/10103835</td>\n",
       "      <td>nla.obj-2060262652</td>\n",
       "      <td>digitised</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>territory-of-papua-annual-report-for-the-perio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review of operations / Australian Land Transpo...</td>\n",
       "      <td>Australian Land Transport Development Program</td>\n",
       "      <td>1991-1994</td>\n",
       "      <td>Government publication | Periodical | Periodic...</td>\n",
       "      <td>https://nla.gov.au/nla.obj-1654948325</td>\n",
       "      <td>https://trove.nla.gov.au/work/10105924</td>\n",
       "      <td>nla.obj-1654948325</td>\n",
       "      <td>digitised</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>review-of-operations-australian-land-transport...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                Laws, etc. (Acts of the Parliament)   \n",
       "1                         The Silver stream songster   \n",
       "2       Report / Defence Force Remuneration Tribunal   \n",
       "3  Territory of Papua : annual report for the per...   \n",
       "4  Review of operations / Australian Land Transpo...   \n",
       "\n",
       "                                      contributor     issued  \\\n",
       "0                                        Victoria  1900-2021   \n",
       "1                                                  1890-1900   \n",
       "2  Australia. Defence Force Remuneration Tribunal  1980-2021   \n",
       "3   Australia. Department of External Territories  1940-1949   \n",
       "4   Australian Land Transport Development Program  1991-1994   \n",
       "\n",
       "                                              format  \\\n",
       "0  Government publication | Periodical | Periodic...   \n",
       "1   Periodical | Periodical/Journal, magazine, other   \n",
       "2  Government publication | Periodical | Periodic...   \n",
       "3  Government publication | Periodical | Periodic...   \n",
       "4  Government publication | Periodical | Periodic...   \n",
       "\n",
       "                            fulltext_url  \\\n",
       "0     http://nla.gov.au/nla.obj-54127737   \n",
       "1   https://nla.gov.au/nla.obj-614066685   \n",
       "2  https://nla.gov.au/nla.obj-2137302489   \n",
       "3  https://nla.gov.au/nla.obj-2060262652   \n",
       "4  https://nla.gov.au/nla.obj-1654948325   \n",
       "\n",
       "                                trove_url            trove_id  \\\n",
       "0  https://trove.nla.gov.au/work/10078182    nla.obj-54127737   \n",
       "1  https://trove.nla.gov.au/work/10087062   nla.obj-614066685   \n",
       "2  https://trove.nla.gov.au/work/10096343  nla.obj-2137302489   \n",
       "3  https://trove.nla.gov.au/work/10103835  nla.obj-2060262652   \n",
       "4  https://trove.nla.gov.au/work/10105924  nla.obj-1654948325   \n",
       "\n",
       "  fulltext_url_type  issues  issues_with_text  \\\n",
       "0         digitised      15                15   \n",
       "1         digitised       1                 1   \n",
       "2         digitised       1                 1   \n",
       "3         digitised       4                 4   \n",
       "4         digitised       3                 3   \n",
       "\n",
       "                                           directory  \n",
       "0   laws-etc-acts-of-the-parliament-nla.obj-54127737  \n",
       "1       the-silver-stream-songster-nla.obj-614066685  \n",
       "2  report-defence-force-remuneration-tribunal-nla...  \n",
       "3  territory-of-papua-annual-report-for-the-perio...  \n",
       "4  review-of-operations-australian-land-transport...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(titles_with_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='digital-journals-with-text.csv' target='_blank'>digital-journals-with-text.csv</a><br>"
      ],
      "text/plain": [
       "/Volumes/Workspace/mycode/glam-workbench/trove-journals/notebooks/digital-journals-with-text.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.to_csv('digital-journals-with-text.csv', index=False)\n",
    "display(FileLink('digital-journals-with-text.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you want to explore data you've already harvested and saved as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('digital-journals-with-text.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a peek inside..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1163, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of journals with OCRd text\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52185"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of issues\n",
    "df['issues'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51928"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of issues with OCRd text\n",
    "df['issues_with_text'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a markdown-formatted list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='digital-journals-with-text.md' target='_blank'>digital-journals-with-text.md</a><br>"
      ],
      "text/plain": [
       "/Volumes/Workspace/mycode/glam-workbench/trove-journals/notebooks/digital-journals-with-text.md"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.sort_values(by=['title'], inplace=True)\n",
    "with open('digital-journals-with-text.md', 'w') as md_file:\n",
    "    md_file.write('# Digitised journals from Trove with OCRd text')\n",
    "    md_file.write('\\n\\nFor harvesting details see [this notebook](Download-text-for-all-digitised-journals.ipynb), or the [digitised journals section](https://glam-workbench.github.io/trove-journals/) of the GLAM Workbench.')\n",
    "    md_file.write(f'\\n\\nThis harvest was completed on {arrow.now(\"Australia/Canberra\").format(\"D MMMM YYYY\")}.')\n",
    "    md_file.write(f'\\n\\nNumber of journals harvested: {df.shape[0]}')\n",
    "    md_file.write(f'\\n\\nNumber of issues with OCRd text: {df[\"issues_with_text\"].sum():,}')\n",
    "    for row in df.itertuples():\n",
    "        md_file.write(f'\\n### {row.title}')\n",
    "        if row.contributor:\n",
    "            md_file.write(f'\\n**{row.contributor}, {row.issued}**')\n",
    "        else:\n",
    "            md_file.write(f'\\n**{row.issued}**')\n",
    "        md_file.write(f'  \\n{row.format}')\n",
    "        md_file.write(f'\\n\\n{row.issues_with_text} of {row.issues} issues have OCRd text available for download.')\n",
    "        md_file.write(f'\\n\\n* [Details on Trove]({row.trove_url})\\n')\n",
    "        md_file.write(f'* [Browse issues on Trove]({row.fulltext_url})\\n')\n",
    "        md_file.write(f'* [Download issue data as CSV from CloudStor](https://cloudstor.aarnet.edu.au/plus/s/QOmnqpGQCNCSC2h/download?path=%2F{row.directory}&files={row.trove_id}-issues.csv)\\n')\n",
    "        md_file.write(f'* [Download all OCRd text from CloudStor](https://cloudstor.aarnet.edu.au/plus/s/QOmnqpGQCNCSC2h/download?path=%2F{row.directory})\\n')\n",
    "        \n",
    "display(FileLink('digital-journals-with-text.md'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/) for the [GLAM Workbench](https://glam-workbench.github.io/).\n",
    "\n",
    "Work on this notebook was supported by the [Humanities, Arts and Social Sciences (HASS) Data Enhanced Virtual Lab](https://tinker.edu.au/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
